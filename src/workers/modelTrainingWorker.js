import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js';
import { workerEvents } from '../events/constants.js';

let _globalCtx = {};
let _model = null;

// ====================================================================
// ðŸ“Š Features numÃ©ricas do dataset de produtividade estudantil
// ====================================================================
// Cada feature representa um aspecto do comportamento/perfil do aluno.
// Todas serÃ£o normalizadas para 0â€“1 antes de entrar na rede neural.
const NUMERIC_FEATURES = [
    'age',                    // Idade do aluno (17â€“30)
    'study_hours_per_day',    // Horas de estudo por dia
    'sleep_hours',            // Horas de sono
    'phone_usage_hours',      // Horas no celular
    'social_media_hours',     // Horas em redes sociais
    'youtube_hours',          // Horas no YouTube
    'gaming_hours',           // Horas jogando
    'breaks_per_day',         // Pausas por dia
    'coffee_intake_mg',       // CafeÃ­na consumida (mg)
    'exercise_minutes',       // Minutos de exercÃ­cio
    'assignments_completed',  // Trabalhos completados
    'attendance_percentage',  // Porcentagem de presenÃ§a
    'stress_level',           // NÃ­vel de estresse (1â€“10)
    'focus_score',            // Score de foco
    'final_grade',            // Nota final
];

// ðŸŽ¯ O que queremos prever:
const TARGET = 'productivity_score';

// ðŸ”¢ Normalizar valores contÃ­nuos para 0â€“1
// Por quÃª? MantÃ©m todas as features balanceadas para o treinamento.
// FÃ³rmula: (val - min) / (max - min)
// Exemplo: coffee=250, min=50, max=500 â†’ (250-50)/(500-50) = 0.44
const normalize = (value, min, max) => (value - min) / ((max - min) || 1);

// ====================================================================
// ðŸ“„ Parser CSV simples
// ====================================================================
// Converte texto CSV em array de objetos JavaScript.
// NÃºmeros sÃ£o convertidos automaticamente, strings permanecem como strings.
function parseCSV(text) {
    const lines = text.trim().split('\n');
    const headers = lines[0].split(',').map(h => h.trim());

    return lines.slice(1).map(line => {
        const values = line.split(',');
        const obj = {};
        headers.forEach((header, i) => {
            const val = values[i]?.trim();
            obj[header] = isNaN(val) || val === '' ? val : Number(val);
        });
        return obj;
    });
}

// ====================================================================
// ðŸ§® Criar contexto de normalizaÃ§Ã£o a partir dos dados
// ====================================================================
// Calcula min/max de cada feature numÃ©rica para normalizaÃ§Ã£o,
// e indexa os valores categÃ³ricos (gender) para one-hot encoding.
//
// ðŸ“Œ Antes: makeContext(products, users) usava preÃ§os e cores de produtos.
//    Agora: makeContext(students) usa features comportamentais de alunos.
function makeContext(students) {
    // Min/max para normalizaÃ§Ã£o de cada feature numÃ©rica
    // Usa reduce em vez de spread (...) para evitar stack overflow com 20k linhas
    const ranges = {};
    NUMERIC_FEATURES.forEach(feat => {
        const values = students.map(s => s[feat]);
        ranges[feat] = {
            min: values.reduce((a, b) => Math.min(a, b), Infinity),
            max: values.reduce((a, b) => Math.max(a, b), -Infinity),
        };
    });

    // Min/max do target (productivity_score)
    const targetValues = students.map(s => s[TARGET]);
    ranges[TARGET] = {
        min: targetValues.reduce((a, b) => Math.min(a, b), Infinity),
        max: targetValues.reduce((a, b) => Math.max(a, b), -Infinity),
    };

    // GÃªneros para one-hot encoding (ex: Male=0, Female=1, Other=2)
    const genders = [...new Set(students.map(s => s.gender))].sort();
    const genderIndex = Object.fromEntries(genders.map((g, i) => [g, i]));

    return {
        students,
        ranges,
        genderIndex,
        genders,
        numGenders: genders.length,
        // Total de dimensÃµes: features numÃ©ricas + one-hot de gÃªnero
        dimensions: NUMERIC_FEATURES.length + genders.length,
    };
}

// ====================================================================
// ðŸ”¤ Codificar um estudante em vetor numÃ©rico
// ====================================================================
// ðŸ“Œ Antes: encodeProduct() + encodeUser() criavam vetores separados
//    com one-hot de categoria/cor e pesos manuais.
//    Agora: encodeStudent() cria um ÃšNICO vetor com todas as features.
//
// Exemplo de vetor resultante para um aluno (Male, 23 anos, etc.):
// [
//   0.55,  // age normalizada
//   0.43,  // study_hours normalizada
//   0.36,  // sleep_hours normalizada
//   ...    // demais features numÃ©ricas normalizadas
//   1, 0, 0  // one-hot de gÃªnero (Female=0, Male=1, Other=0)
// ]
function encodeStudent(student, context) {
    // Normalizar todas as features numÃ©ricas para 0â€“1
    const numericValues = NUMERIC_FEATURES.map(feat =>
        normalize(student[feat], context.ranges[feat].min, context.ranges[feat].max)
    );

    const numeric = tf.tensor1d(numericValues);

    // One-hot encoding do gÃªnero
    const gender = tf.oneHot(
        context.genderIndex[student.gender],
        context.numGenders
    ).cast('float32');

    return tf.concat1d([numeric, gender]);
}

// ====================================================================
// ðŸ“¦ Criar dados de treinamento
// ====================================================================
// ðŸ“Œ Antes: createTrainingData() fazia cross-join user Ã— product,
//    gerando pares [userVector, productVector] â†’ label (comprou/nÃ£o).
//    Agora: cada LINHA do CSV = 1 exemplo de treino.
//    Input: features do aluno â†’ Label: productivity_score normalizado.
function createTrainingData(context) {
    const inputs = [];
    const labels = [];

    context.students.forEach(student => {
        const vector = encodeStudent(student, context).dataSync();
        inputs.push([...vector]);
        // Normalizar o target (productivity_score) para 0â€“1
        labels.push(
            normalize(student[TARGET], context.ranges[TARGET].min, context.ranges[TARGET].max)
        );
    });

    return {
        xs: tf.tensor2d(inputs),
        ys: tf.tensor2d(labels, [labels.length, 1]),
        inputDimension: context.dimensions,
    };
}

// ====================================================================
// ðŸ§  ConfiguraÃ§Ã£o e treinamento da rede neural
// ====================================================================
// ðŸ“Œ MudanÃ§as em relaÃ§Ã£o ao e-commerce:
//    - Loss: binaryCrossentropy â†’ meanSquaredError (REGRESSÃƒO)
//    - MÃ©trica: accuracy â†’ mae (Mean Absolute Error)
//    - AtivaÃ§Ã£o final: sigmoid (target normalizado 0â€“1)
//    - Adicionado Dropout para evitar overfitting (20k amostras)
//    - ValidaÃ§Ã£o split 80/20
async function configureNeuralNetAndTrain(trainData) {
    const model = tf.sequential();

    // Camada de entrada
    // - inputShape: nÃºmero de features (15 numÃ©ricas + 3 one-hot de gÃªnero = 18)
    // - units: 128 neurÃ´nios para detectar padrÃµes iniciais
    // - activation: 'relu' (mantÃ©m apenas sinais positivos)
    model.add(
        tf.layers.dense({
            inputShape: [trainData.inputDimension],
            units: 128,
            activation: 'relu'
        })
    );
    // Dropout: desliga 20% dos neurÃ´nios aleatoriamente a cada passo
    // Por quÃª? Com 20k amostras, ajuda a evitar overfitting
    model.add(tf.layers.dropout({ rate: 0.2 }));

    // Camada oculta 1 â€” 64 neurÃ´nios (comprimindo informaÃ§Ã£o)
    model.add(
        tf.layers.dense({
            units: 64,
            activation: 'relu'
        })
    );
    model.add(tf.layers.dropout({ rate: 0.2 }));

    // Camada oculta 2 â€” 32 neurÃ´nios (destilando padrÃµes mais fortes)
    model.add(
        tf.layers.dense({
            units: 32,
            activation: 'relu'
        })
    );

    // Camada de saÃ­da
    // - 1 neurÃ´nio: retorna o score de produtividade previsto
    // - sigmoid: comprime para 0â€“1 (compatÃ­vel com target normalizado)
    model.add(
        tf.layers.dense({ units: 1, activation: 'sigmoid' })
    );

    // ðŸ“Œ MudanÃ§a crÃ­tica: MSE para regressÃ£o (antes era binaryCrossentropy)
    model.compile({
        optimizer: tf.train.adam(0.001),
        loss: 'meanSquaredError',
        metrics: ['mae'] // Mean Absolute Error â€” erro mÃ©dio absoluto
    });

    // Separar dados em treino (80%) e validaÃ§Ã£o (20%)
    const numSamples = trainData.xs.shape[0];
    const splitIndex = Math.floor(numSamples * 0.8);

    const xTrain = trainData.xs.slice(0, splitIndex);
    const yTrain = trainData.ys.slice(0, splitIndex);
    const xVal = trainData.xs.slice(splitIndex);
    const yVal = trainData.ys.slice(splitIndex);

    await model.fit(xTrain, yTrain, {
        epochs: 50,
        batchSize: 64,
        validationData: [xVal, yVal],
        shuffle: true,
        callbacks: {
            onEpochEnd: (epoch, logs) => {
                postMessage({
                    type: workerEvents.trainingLog,
                    epoch: epoch,
                    loss: logs.loss,
                    mae: logs.mae,
                    val_loss: logs.val_loss,
                    val_mae: logs.val_mae,
                });
            }
        }
    });

    return model;
}

// ====================================================================
// ðŸš€ Treinar o modelo
// ====================================================================
// ðŸ“Œ Antes: recebia users e buscava products.json.
//    Agora: busca students.csv e parseia diretamente no worker.
async function trainModel() {
    console.log('Training model with student productivity dataset...');
    postMessage({ type: workerEvents.progressUpdate, progress: { progress: 1 } });

    // Carregar e parsear o CSV (20.000 alunos)
    const csvText = await (await fetch('/data/students.csv')).text();
    const students = parseCSV(csvText);
    console.log(`Loaded ${students.length} students from CSV`);

    const context = makeContext(students);
    _globalCtx = context;

    const trainData = createTrainingData(context);
    _model = await configureNeuralNetAndTrain(trainData);

    postMessage({ type: workerEvents.progressUpdate, progress: { progress: 100 } });
    postMessage({ type: workerEvents.trainingComplete });
}

// ====================================================================
// ðŸ”® Prever produtividade de um aluno
// ====================================================================
// ðŸ“Œ Antes: recommend() calculava scores de compatibilidade userÃ—product.
//    Agora: predict() recebe features de um aluno e retorna o
//    productivity_score previsto pela rede neural.
//
// Fluxo:
// 1. Codificar as features do aluno em vetor numÃ©rico
// 2. Passar pelo modelo treinado
// 3. Desnormalizar o resultado para a escala original (0â€“100)
// 4. Enviar resultado de volta para a UI
function predict({ student }) {
    if (!_model) return;
    const context = _globalCtx;

    // 1ï¸âƒ£ Codificar as features do aluno no mesmo formato do treino
    const vector = encodeStudent(student, context);
    const inputTensor = vector.reshape([1, context.dimensions]);

    // 2ï¸âƒ£ Rodar a previsÃ£o
    const prediction = _model.predict(inputTensor);

    // 3ï¸âƒ£ Desnormalizar: resultado 0â€“1 â†’ escala original
    const normalizedScore = prediction.dataSync()[0];
    const { min, max } = context.ranges[TARGET];
    const score = normalizedScore * (max - min) + min;

    // 4ï¸âƒ£ Enviar para a thread principal
    postMessage({
        type: workerEvents.predict,
        student,
        predictedProductivity: Math.round(score * 100) / 100,
    });
}

// ====================================================================
// ðŸ“¡ Handler de mensagens do worker
// ====================================================================
const handlers = {
    [workerEvents.trainModel]: trainModel,
    [workerEvents.predict]: predict,
};

self.onmessage = e => {
    const { action, ...data } = e.data;
    if (handlers[action]) handlers[action](data);
};
